{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets Desbalanceados\n",
    "\n",
    "## 1. Datasets Sintéticos\n",
    "\n",
    "Vamos a comenzar generando un dataset sintético. La ventaja de este enfoque es que podemos controlar muchas características de este dataset. Por ejemplo, la cantidad de features, si hay features correlacionados o no, la separación entre clases, el desbalanceo, etc.\n",
    "\n",
    "Vamos a comenzar generando un dataset, que luego separaremos en un dataset medido y en un dataset no medido. De esta forma, simulamos (de una manera muy inocente) el proceso de medición. Esto se podría hacer mejor: en este proceso de medición podríamos agregar ruido, valores mal medidos, etiquetas intercambiadas, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda genera los datos con los que vamos a trabajar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_real, y_real = make_classification(n_samples=100000,n_features=4, n_informative=4,\n",
    "                                     n_redundant=0, n_clusters_per_class=1,\n",
    "                                     class_sep=1.0, weights = [0.99], random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y miramos la cantidad de instancias con etiqueta positiva y qué porcentaje del dataset representa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_real.sum())\n",
    "print(y_real.sum()/y_real.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos a un DataFrame de Pandas para poder aprovechar algunas funcionalidades de la librería."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_real.shape[1]):\n",
    "    df_real['x' + str(i)] = X_real[:,i]\n",
    "df_real['y'] = y_real  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ya viene mezclado al azar, seleccionar las diez mil primeras instancias es equivalente a muestrear al azar el dataset original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "df_medido = df_real[:N]\n",
    "df_medido.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y dejamos el resto de los los datos como instancias 'no medidas'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_medido = df_real[N:].reset_index(drop = True)\n",
    "df_no_medido.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuántas instancias positivas y qué porcentaje hay en cada dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_medido.y.sum())\n",
    "print(df_medido.y.sum()/df_medido.size)\n",
    "\n",
    "print(df_no_medido.y.sum())\n",
    "print(df_no_medido.y.sum()/df_no_medido.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración de los datos\n",
    "\n",
    "Miremos cómo es el dataset con el que vamos a trabajar, `df_medido`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data = df_medido, vars = df_medido.columns[:-1], hue = 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y cómo queda la tabla de correlaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_medido.corr('pearson')\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15},\n",
    "           xticklabels= df_medido.columns, \n",
    "           yticklabels= df_medido.columns,\n",
    "           cmap= 'coolwarm')\n",
    "# plt.xticks(rotation = 45)\n",
    "# plt.yticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuáles atributos serán buenos predictores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:** Familiarizarse con la función que genera los datos. Cambiar algunos de sus parámetros y volver a correr. \n",
    "\n",
    "**Para pensar**: ¿Qué pasa con la tabla de correlaciones a medida que la prevalencia de la clase positiva disminuye?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entrenamiento Modelo Uno\n",
    "\n",
    "Vamos a entrenar un primer modelo de árbol de decisión y evaluarlo usando exactitud. Para ello:\n",
    "\n",
    "Seleccionamos variables predictoras y etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_medido.drop('y', axis = 1).values\n",
    "y = df_medido.y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos un `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos cómo son las distribuciones de las variables predictoras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_train.shape[1]):\n",
    "    sns.distplot(X_train[:,i])\n",
    "    sns.distplot(X_test[:,i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y la proporción de etiquetas positivas en los datos de train y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Proporcion de etiquetas positiva en los datos de Train: ', y_train.sum()/y_train.size)\n",
    "print('Proporcion de etiquetas positiva en los datos de Test: ', y_test.sum()/y_test.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo\n",
    "\n",
    "Vamos a hacer una curva de validación para elegir la mejor profundidad para el árbol de decisión. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "tree_train_scores_mean = []\n",
    "tree_train_scores_std = []\n",
    "tree_test_scores_mean = []\n",
    "tree_test_scores_std = []\n",
    "\n",
    "profundidades = np.arange(1,50,1)\n",
    "\n",
    "for profundidad in profundidades:\n",
    "    clf = DecisionTreeClassifier(max_depth=profundidad, random_state=42)\n",
    "    tree_scores = cross_validate(clf, X_train, y_train, cv=5, return_train_score=True, n_jobs = -1)\n",
    "    \n",
    "    tree_train_scores_mean.append(tree_scores['train_score'].mean())\n",
    "    tree_train_scores_std.append(tree_scores['train_score'].std())\n",
    "    \n",
    "    tree_test_scores_mean.append(tree_scores['test_score'].mean())\n",
    "    tree_test_scores_std.append(tree_scores['test_score'].std())\n",
    "\n",
    "tree_train_scores_mean = np.array(tree_train_scores_mean)\n",
    "tree_train_scores_std = np.array(tree_train_scores_std)\n",
    "tree_test_scores_mean = np.array(tree_test_scores_mean)\n",
    "tree_test_scores_std = np.array(tree_test_scores_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.fill_between(profundidades, tree_train_scores_mean - tree_train_scores_std,\n",
    "                 tree_train_scores_mean + tree_train_scores_std, alpha=0.1,\n",
    "                 color=\"r\")\n",
    "plt.fill_between(profundidades, tree_test_scores_mean - tree_test_scores_std,\n",
    "                 tree_test_scores_mean + tree_test_scores_std, alpha=0.1, color=\"g\")\n",
    "\n",
    "plt.plot(profundidades, tree_train_scores_mean, 'o-', color=\"r\",\n",
    "         label=\"Training score\")\n",
    "plt.plot(profundidades, tree_test_scores_mean, 'o-', color=\"g\",\n",
    "         label=\"Test score\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Profundidad Arbol de Decision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuál profundidad usarían? ¿Cuál es el *benchmark* de este problema?\n",
    "\n",
    "Entrenemos un árbol de profundidad tres y evaluémoslo en el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_1 = DecisionTreeClassifier(max_depth = 3, random_state = 42)\n",
    "clf_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecimos sobre nuestro set de entrenamieto\n",
    "y_train_pred = clf_1.predict(X_train)\n",
    "\n",
    "# Predecimos sobre nuestro set de test\n",
    "y_test_pred = clf_1.predict(X_test)\n",
    "\n",
    "# Comaparamos con las etiquetas reales\n",
    "print('Accuracy sobre conjunto de Train:', accuracy_score(y_train_pred,y_train))\n",
    "print('Accuracy sobre conjunto de Test:', accuracy_score(y_test_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Es un buen modelo? Veamos la matriz de confusión en cada conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train, y_train_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuáles son sus aciertos, Falsos Positivos y Falsos Negativos?¿Es lo mismo si nos interesa la clase 0 que la clase 1? En el caso de un examen médico, ¿un FP tiene el mismo costo que un FN?\n",
    "\n",
    "**Ejercicio:** calcular la precisión, exhaustividad (recall) y F-Score de este modelo para cada clase sobre el conjunto de Test. Pueden hacerlo a partir de la matriz de confusión o usando funciones que ya están incorporadas en Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "print(precision_recall_fscore_support(y_test, y_test_pred, average='macro'))\n",
    "print(precision_recall_fscore_support(y_test, y_test_pred, average='micro'))\n",
    "print(precision_recall_fscore_support(y_test, y_test_pred, average='weighted'))\n",
    "print(precision_recall_fscore_support(y_test, y_test_pred, average='binary', pos_label = 0))\n",
    "print(precision_recall_fscore_support(y_test, y_test_pred, average='binary', pos_label = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Y si lo ponemos \"en producción\"?\n",
    "\n",
    "Una de las ventajas de trabajar con datos sintéticos es que podemos ver cómo desempeñaría nuestro modelo si lo ponemos en producción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_no_medido = df_no_medido.drop('y', axis = 1).values\n",
    "y_no_medido = df_no_medido.y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecimos sobre todas las instancias que no vio\n",
    "y_no_medido_pred = clf_1.predict(X_no_medido)\n",
    "\n",
    "# Comaparamos con las etiquetas reales\n",
    "print('Accuracy sobre conjunto de Train:', accuracy_score(y_no_medido_pred,y_no_medido))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_no_medido, y_no_medido_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:** medir precisión, exhaustividad y F-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision_recall_fscore_support(y_no_medido, y_no_medido_pred, average='macro'))\n",
    "print(precision_recall_fscore_support(y_no_medido, y_no_medido_pred, average='micro'))\n",
    "print(precision_recall_fscore_support(y_no_medido, y_no_medido_pred, average='weighted'))\n",
    "print(precision_recall_fscore_support(y_no_medido, y_no_medido_pred, average='binary', pos_label = 0))\n",
    "print(precision_recall_fscore_support(y_no_medido, y_no_medido_pred, average='binary', pos_label = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:** repetir para un modelo de vecinos más cercanos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_train_scores_mean = []\n",
    "knn_train_scores_std = []\n",
    "knn_test_scores_mean = []\n",
    "knn_test_scores_std = []\n",
    "\n",
    "n_vecinos = np.arange(1,50,1)\n",
    "\n",
    "for vecinos in n_vecinos:\n",
    "    clf = KNeighborsClassifier(n_neighbors=vecinos)\n",
    "    knn_scores = cross_validate(clf, X_train, y_train, cv=5, return_train_score=True, n_jobs = -1)\n",
    "    \n",
    "    knn_train_scores_mean.append(knn_scores['train_score'].mean())\n",
    "    knn_train_scores_std.append(knn_scores['train_score'].std())\n",
    "    \n",
    "    knn_test_scores_mean.append(knn_scores['test_score'].mean())\n",
    "    knn_test_scores_std.append(knn_scores['test_score'].std())\n",
    "\n",
    "knn_train_scores_mean = np.array(knn_train_scores_mean)\n",
    "knn_train_scores_std = np.array(knn_train_scores_std)\n",
    "knn_test_scores_mean = np.array(knn_test_scores_mean)\n",
    "knn_test_scores_std = np.array(knn_test_scores_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.fill_between(n_vecinos, knn_train_scores_mean - knn_train_scores_std,\n",
    "                 knn_train_scores_mean + knn_train_scores_std, alpha=0.1,\n",
    "                 color=\"r\")\n",
    "plt.fill_between(n_vecinos, knn_test_scores_mean - knn_test_scores_std,\n",
    "                 knn_test_scores_mean + knn_test_scores_std, alpha=0.1, color=\"g\")\n",
    "\n",
    "plt.plot(n_vecinos, knn_train_scores_mean, 'o-', color=\"r\",\n",
    "         label=\"Training score\")\n",
    "plt.plot(n_vecinos, knn_test_scores_mean, 'o-', color=\"g\",\n",
    "         label=\"Test score\")\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Cantidad de Vecinos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_1_knn = KNeighborsClassifier()\n",
    "clf_1_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecimos sobre nuestro set de entrenamieto\n",
    "y_train_pred = clf_1_knn.predict(X_train)\n",
    "\n",
    "# Predecimos sobre nuestro set de test\n",
    "y_test_pred = clf_1_knn.predict(X_test)\n",
    "\n",
    "# Comaparamos con las etiquetas reales\n",
    "print('Accuracy sobre conjunto de Train:', accuracy_score(y_train_pred,y_train))\n",
    "print('Accuracy sobre conjunto de Test:', accuracy_score(y_test_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision_recall_fscore_support(y_test, y_test_pred, average='macro'))\n",
    "print(precision_recall_fscore_support(y_test, y_test_pred, average='micro'))\n",
    "print(precision_recall_fscore_support(y_test, y_test_pred, average='weighted'))\n",
    "print(precision_recall_fscore_support(y_test, y_test_pred, average='binary', pos_label = 0))\n",
    "print(precision_recall_fscore_support(y_test, y_test_pred, average='binary', pos_label = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puesta en producción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecimos sobre todas las instancias que no vio\n",
    "y_no_medido_pred = clf_1_knn.predict(X_no_medido)\n",
    "\n",
    "# Comaparamos con las etiquetas reales\n",
    "print('Accuracy sobre conjunto de Train:', accuracy_score(y_no_medido_pred,y_no_medido))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_no_medido, y_no_medido_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision_recall_fscore_support(y_no_medido, y_no_medido_pred, average='macro'))\n",
    "print(precision_recall_fscore_support(y_no_medido, y_no_medido_pred, average='micro'))\n",
    "print(precision_recall_fscore_support(y_no_medido, y_no_medido_pred, average='weighted'))\n",
    "print(precision_recall_fscore_support(y_no_medido, y_no_medido_pred, average='binary', pos_label = 0))\n",
    "print(precision_recall_fscore_support(y_no_medido, y_no_medido_pred, average='binary', pos_label = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Balanceando el Dataset\n",
    "\n",
    "Vamos a balancear el dataset subsampleando la clase más prevalente. Luego, volvemos a analizar los datos y entrenar los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_medido.y == 1\n",
    "\n",
    "df_subsample = pd.concat([df_medido[mask], df_medido[~mask].sample(n = mask.sum())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subsample = df_subsample.sample(frac=1,  random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos el `pairplot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data = df_subsample, vars = df_subsample.columns[:-1], hue = 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y la tabla de correlaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_subsample.corr('pearson')\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15},\n",
    "           xticklabels= df_subsample.columns, \n",
    "           yticklabels= df_subsample.columns,\n",
    "           cmap= 'coolwarm')\n",
    "# plt.xticks(rotation = 45)\n",
    "# plt.yticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Notan algo diferente en la tabla con respecto a la anterior? Si quieren, copien la celda de código de la tabla anterior para poder verlas juntas.\n",
    "\n",
    "## 4. Entrenamiento Modelo Dos\n",
    "\n",
    "Seleccionamos variables predictoras y etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_subsample.drop('y', axis = 1).values\n",
    "y = df_subsample.y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos cómo son las distribuciones de las variables predictoras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_train.shape[1]):\n",
    "    sns.distplot(X_train[:,i])\n",
    "    sns.distplot(X_test[:,i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y la proporción de etiquetas positivas en los datos de train y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Proporcion de etiquetas positiva en los datos de Train: ', y_train.sum()/y_train.size)\n",
    "print('Proporcion de etiquetas positiva en los datos de Test: ', y_test.sum()/y_test.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo\n",
    "\n",
    "Volvemos a hacer la curva de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_train_scores_mean = []\n",
    "tree_train_scores_std = []\n",
    "tree_test_scores_mean = []\n",
    "tree_test_scores_std = []\n",
    "\n",
    "profundidades = np.arange(1,50,1)\n",
    "\n",
    "for profundidad in profundidades:\n",
    "    clf = DecisionTreeClassifier(max_depth=profundidad, random_state=42)\n",
    "    tree_scores = cross_validate(clf, X_train, y_train, cv=5, return_train_score=True, n_jobs = -1)\n",
    "    \n",
    "    tree_train_scores_mean.append(tree_scores['train_score'].mean())\n",
    "    tree_train_scores_std.append(tree_scores['train_score'].std())\n",
    "    \n",
    "    tree_test_scores_mean.append(tree_scores['test_score'].mean())\n",
    "    tree_test_scores_std.append(tree_scores['test_score'].std())\n",
    "\n",
    "tree_train_scores_mean = np.array(tree_train_scores_mean)\n",
    "tree_train_scores_std = np.array(tree_train_scores_std)\n",
    "tree_test_scores_mean = np.array(tree_test_scores_mean)\n",
    "tree_test_scores_std = np.array(tree_test_scores_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.fill_between(profundidades, tree_train_scores_mean - tree_train_scores_std,\n",
    "                 tree_train_scores_mean + tree_train_scores_std, alpha=0.1,\n",
    "                 color=\"r\")\n",
    "plt.fill_between(profundidades, tree_test_scores_mean - tree_test_scores_std,\n",
    "                 tree_test_scores_mean + tree_test_scores_std, alpha=0.1, color=\"g\")\n",
    "\n",
    "plt.plot(profundidades, tree_train_scores_mean, 'o-', color=\"r\",\n",
    "         label=\"Training score\")\n",
    "plt.plot(profundidades, tree_test_scores_mean, 'o-', color=\"g\",\n",
    "         label=\"Test score\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Profundidad Arbol de Decision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuál profunidad usarían? Cuál es el *benchmark* de este problema?\n",
    "\n",
    "Entrenemos un árbol de profundidad tres y evaluémoslo en el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_2 = DecisionTreeClassifier(max_depth = 3, random_state = 42)\n",
    "clf_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecimos sobre nuestro set de entrenamieto\n",
    "y_train_pred = clf_2.predict(X_train)\n",
    "\n",
    "# Predecimos sobre nuestro set de test\n",
    "y_test_pred = clf_2.predict(X_test)\n",
    "\n",
    "# Comaparamos con las etiquetas reales\n",
    "print('Accuracy sobre conjunto de Train:', accuracy_score(y_train_pred,y_train))\n",
    "print('Accuracy sobre conjunto de Test:', accuracy_score(y_test_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Es un buen modelo? Veamos la matriz de confusión en cada conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuáles son sus aciertos, Falsos Positivos y Falsos Negativos?¿Es lo mismo si nos interesa la clase 0 que la clase 1? \n",
    "\n",
    "**Ejercicio:** Igual que antes. Calcular la precisión, exhaustividad (recall) y F-Score de este modelo para cada clase sobre el conjunto de Test. Pueden hacerlo a partir de la matriz de confusión o usando funciones que ya están incorporadas en Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision_recall_fscore_support(y_test, y_test_pred, average='macro'))\n",
    "print(precision_recall_fscore_support(y_test, y_test_pred, average='micro'))\n",
    "print(precision_recall_fscore_support(y_test, y_test_pred, average='weighted'))\n",
    "print(precision_recall_fscore_support(y_test, y_test_pred, average='binary', pos_label = 0))\n",
    "print(precision_recall_fscore_support(y_test, y_test_pred, average='binary', pos_label = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Y si lo ponemos \"en producción\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_no_medido = df_no_medido.drop('y', axis = 1).values\n",
    "y_no_medido = df_no_medido.y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecimos sobre todas las instancias que no vio\n",
    "y_no_medido_pred = clf_2.predict(X_no_medido)\n",
    "\n",
    "# Comaparamos con las etiquetas reales\n",
    "print('Accuracy sobre conjunto de Train:', accuracy_score(y_no_medido_pred,y_no_medido))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_no_medido, y_no_medido_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué cambió?¿Es mejor o peor este modelo que el anterior árbol de decisión?\n",
    "\n",
    "**Ejercicio:** medir precisión, exhaustividad y F-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision_recall_fscore_support(y_no_medido, y_no_medido_pred, average='macro'))\n",
    "print(precision_recall_fscore_support(y_no_medido, y_no_medido_pred, average='micro'))\n",
    "print(precision_recall_fscore_support(y_no_medido, y_no_medido_pred, average='weighted'))\n",
    "print(precision_recall_fscore_support(y_no_medido, y_no_medido_pred, average='binary', pos_label = 0))\n",
    "print(precision_recall_fscore_support(y_no_medido, y_no_medido_pred, average='binary', pos_label = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:** repetir para un modelo de vecinos más cercanos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_train_scores_mean = []\n",
    "knn_train_scores_std = []\n",
    "knn_test_scores_mean = []\n",
    "knn_test_scores_std = []\n",
    "\n",
    "n_vecinos = np.arange(1,50,1)\n",
    "\n",
    "for vecinos in n_vecinos:\n",
    "    clf = KNeighborsClassifier(n_neighbors=vecinos)\n",
    "    knn_scores = cross_validate(clf, X_train, y_train, cv=5, return_train_score=True, n_jobs = -1)\n",
    "    \n",
    "    knn_train_scores_mean.append(knn_scores['train_score'].mean())\n",
    "    knn_train_scores_std.append(knn_scores['train_score'].std())\n",
    "    \n",
    "    knn_test_scores_mean.append(knn_scores['test_score'].mean())\n",
    "    knn_test_scores_std.append(knn_scores['test_score'].std())\n",
    "\n",
    "knn_train_scores_mean = np.array(knn_train_scores_mean)\n",
    "knn_train_scores_std = np.array(knn_train_scores_std)\n",
    "knn_test_scores_mean = np.array(knn_test_scores_mean)\n",
    "knn_test_scores_std = np.array(knn_test_scores_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.fill_between(n_vecinos, knn_train_scores_mean - knn_train_scores_std,\n",
    "                 knn_train_scores_mean + knn_train_scores_std, alpha=0.1,\n",
    "                 color=\"r\")\n",
    "plt.fill_between(n_vecinos, knn_test_scores_mean - knn_test_scores_std,\n",
    "                 knn_test_scores_mean + knn_test_scores_std, alpha=0.1, color=\"g\")\n",
    "\n",
    "plt.plot(n_vecinos, knn_train_scores_mean, 'o-', color=\"r\",\n",
    "         label=\"Training score\")\n",
    "plt.plot(n_vecinos, knn_test_scores_mean, 'o-', color=\"g\",\n",
    "         label=\"Test score\")\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Cantidad de Vecinos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_2_knn = KNeighborsClassifier(n_neighbors=14)\n",
    "clf_2_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecimos sobre nuestro set de entrenamieto\n",
    "y_train_pred = clf_2_knn.predict(X_train)\n",
    "\n",
    "# Predecimos sobre nuestro set de test\n",
    "y_test_pred = clf_2_knn.predict(X_test)\n",
    "\n",
    "# Comaparamos con las etiquetas reales\n",
    "print('Accuracy sobre conjunto de Train:', accuracy_score(y_train_pred,y_train))\n",
    "print('Accuracy sobre conjunto de Test:', accuracy_score(y_test_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision_recall_fscore_support(y_test, y_test_pred, average='macro'))\n",
    "print(precision_recall_fscore_support(y_test, y_test_pred, average='micro'))\n",
    "print(precision_recall_fscore_support(y_test, y_test_pred, average='weighted'))\n",
    "print(precision_recall_fscore_support(y_test, y_test_pred, average='binary', pos_label = 0))\n",
    "print(precision_recall_fscore_support(y_test, y_test_pred, average='binary', pos_label = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puesta en producción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecimos sobre todas las instancias que no vio\n",
    "y_no_medido_pred = clf_2_knn.predict(X_no_medido)\n",
    "\n",
    "# Comaparamos con las etiquetas reales\n",
    "print('Accuracy sobre conjunto de Train:', accuracy_score(y_no_medido_pred,y_no_medido))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_no_medido, y_no_medido_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision_recall_fscore_support(y_no_medido, y_no_medido_pred, average='macro'))\n",
    "print(precision_recall_fscore_support(y_no_medido, y_no_medido_pred, average='micro'))\n",
    "print(precision_recall_fscore_support(y_no_medido, y_no_medido_pred, average='weighted'))\n",
    "print(precision_recall_fscore_support(y_no_medido, y_no_medido_pred, average='binary', pos_label = 0))\n",
    "print(precision_recall_fscore_support(y_no_medido, y_no_medido_pred, average='binary', pos_label = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset de Fraude\n",
    "\n",
    "Los invitamos a trabajar con este dataset: https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "\n",
    "Pueden encontrar un link a un lindo análisis en la presentación de la Clase 21."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Curva ROC\n",
    "Use las funciones `model.predict_proba`, `sklearn.metrics.roc_curve` y `sklearn.metrics.roc_auc_score` de Scikit-learn para realizar los siguientes ejercicios.\n",
    "\n",
    "**Ejecicios**\n",
    "\n",
    "1. Elija de \"Entrenamiento Modelo Uno\" (punto 2) y \"Entrenamiento Modelo Dos\" (punto 4), los mejores modelos según esa métrica (aquellos con el hiperparámetro que performen mejor en el test set).\n",
    "2. Para cada uno de estos dos modelos (mejor modelo UNO y mejor modelo DOS), genere las curvas ROC correspondientes. Grafiquelas y compárelas.\n",
    "3. Para ambos modelos calcule el valor del AUC. Compare el resultado de esta métrica con la de accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimización de Hiperparámetros\n",
    "\n",
    "Use las funciones `GridSearchCV` y `RandomizedSearchCV` de Scikit-learn para realizar los siguientes ejercicios. No olvide explorar su resultados y escribir las conclusiones a las que llegue.\n",
    "\n",
    "**Ejecicios**\n",
    "1. Explore el espacio de hiperparámetros con Grid Search de un árbol de decisión entrenado con el dataset sin balancear (\"Entrenamiento Modelo Uno\") y elija aquellos parámetros que maximicen exactitud. Luego, evalúe la performance en el conjunto de Test y compare con la obtenida por Grid Search. ¿Son diferentes? Si lo son, ¿a qué se debe? Si no lo son, ¿a qué se debe?. Algunas recomendaciones que pueden ser útiles:\n",
    "   1. Recuerde que el espacio a explorar es definido a través de un diccionario. Algunas variables que puede ser interesante explorar, en el caso del árbol de decisión, son: `criterion`, `max_depth`, `min_samples_split` y `min_samples_leaf`.\n",
    "   1. Los resultados del `GridSearchCV` están en un diccionario que se accede con `.cv_results_`. Si quieren conocer las *llaves* de ese diccionario, pueden usar `.cv_results_.keys()`\n",
    "   1. `GridSearchCV` entrena al final un modelo con todo el conjunto de Train con los mejores parámetros que encontró. Se puede usar ese modelo para predecir con `.predict()`\n",
    "   1. Les recomendamos tener a mano la [documentación](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) de `GridSearchCV` en Scikit-Learn.\n",
    "    \n",
    "1. Repita, pero esta evaluando precision, exhaustividad, F-Score y AUC ROC. **Notar** que se pueden evaluar múltiples métricas a la vez. También notar que si no eligen una métrica por sobre las otras, `GridSearchCV` no puede reentrenar con el mejor modelo. ¿Cómo son los hiperparámetros que maximizan cada métrica? Por ejemplo, compare entre precisión y exhaustividad.\n",
    "1. Repita para el dataset balanceado. \n",
    "1. Elija alguno de los casos anteriores y repita, pero esta vez usando Random Search.\n",
    "1. Si aún tiene tiempo y ganas, repita para un clasificador KNN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta celda que les dejamos convierte los resultados del Grid Searh en un DataFrame de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(clf.cv_results_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
