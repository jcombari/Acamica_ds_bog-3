{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistemas de Recomendación - Netflix Prize Challenge\n",
    "\n",
    "En este notebook vamos a implementar un sistema de recomendación a través de un filtro colaborativo.\n",
    "\n",
    "Algunas referencias útiles, además de las mencionadas en la presentación:\n",
    "* https://www.kaggle.com/ibtesama/getting-started-with-a-movie-recommendation-system\n",
    "* https://www.kaggle.com/gspmoreira/recommender-systems-in-python-101\n",
    "\n",
    "El dataset puede ser descargado [acá](https://www.kaggle.com/netflix-inc/netflix-prize-data)\n",
    "\n",
    "Importante leer el archivo `README.md` para la descripción de los archivos.\n",
    "\n",
    "## **Big Data**\n",
    "\n",
    "\n",
    "Uno de los desafíos que plantea este dataset es que es bastante \"grande\". Esto quiere decir que, si lo cargamos completo, ocupa bastante lugar en nuestra memoria RAM. Además, cada tarea puede llevar mucho tiempo. Entonces, es necesario plantear una estrategia para abordarlo. Existen varias posibilidades, mencionamos algunas:\n",
    "1. Recortar una parte del dataset con la que sí podamos trabajar. Esta parte tiene que ser lo suficientemente representativa del set original. Para estar seguros de ello es fundamental hacer una buena exploración de datos. Con ese recorte, entrenamos y evaluamos nuestro modelo, y optimizamos parámetros (CV). Una vez que ya estamos seguro de que nuestro flujo de trabajo es apropiado, podemos probar agrandar la porción de datos con la que entrenamos o utilizar otro recorte del dataset. Eventualmente, podemos llegar a usar todo el dataset para entrenar y evaluar si nuestra computadora lo permite. **NOTA**: el recorte se hace para que se pueda cargar en memoria los datos pero también para que cada iteración lleve un tiempo razonable.\n",
    "2. **Aprendizaje incremental**: algunos modelos puede ser entrenados mostrándoles el dataset de a pedazos. Es decir, no necesitan ver todo el dataset a la vez. Un ejemplo son las redes neuronales, que \"ven\" muchas pasadas del dataset en *epochs* y *minibatches*. Algunos modelos en Scikit-learn tienen la función `partial_fit` que permite hacer eso. Pueden leer un poco al respecto [acá](https://scikit-learn.org/stable/modules/computing.html). Pandas también tiene funciones que permiten cargar el dataset de a trozos.\n",
    "3. Utilizar servicios en la nube. Esta opción no es excluyente con las anteriores. Antes de utilizar algún entorno en la nube, está bueno haber hecho pruebas en nuestra computadora y ya haber optimizado bastante el flujo de trabajo. Recuerden que los servicios en la nube se pagan.\n",
    "4. Existen librerías orientadas a trabajar con grandes datos. Un ejemplo es [Dask](https://dask.org/).\n",
    "\n",
    "## 0. Algunos preliminares\n",
    "\n",
    "Mientras miran el estado de la memoria RAM, crear un arreglo 2-D de unos en numpy de forma (10000,10000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "unos = COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del unos # Borra la variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué tipo de dato es `ones`?¿Y sus elementos? Crear el mismo arreglo, pero convertir los elementos en *np.int8*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unos = COMPLETAR.astype(COMPLETAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por las dudas reiniciar el Kernel antes de continuar y correr a partir de la sección siguiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de Datos y preparación del Dataset\n",
    "\n",
    "Vamos a empezar cargando uno de los archivos con calificaciones para explorarlo. Como son archivos grandes y van a ocupar bastante lugar en memoria, no vamos a cargar la última columna con fechas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import gc #garbage collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(name):\n",
    "    df = pd.read_csv(name, header = None, names = ['User','Rating'], usecols = [0,1])\n",
    "    \n",
    "    # A veces forzar un tipo de dato hace que se ahorre mucho lugar en memoria.\n",
    "    df['Rating'] = df['Rating'].astype(float) \n",
    "    return df\n",
    "\n",
    "\n",
    "df1 = load_data('Datasets/netflix-prize-data/combined_data_1.txt')\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo sabemos a qué película corresponde cada calificación? Contar cuántas películas hay en `df1` e identificarlas. Para ello, cargamos `movie_titles.csv`. Como no nos interesa el año, no lo traemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title = pd.read_csv('Datasets/netflix-prize-data/movie_titles.csv', encoding = \"ISO-8859-1\",index_col = 0, header = None, usecols = [0,2], names = ['Movie_Id', 'Name'])\n",
    "df_title.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma, podemos obtener el nombre de una película dado su Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id = 1\n",
    "print(df_title.loc[movie_id].Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para contar cuántos identificadores hay, vamos a usar la siguiente información: al lado del identificador de la película, la columna `Rating` de `df1` tiene un `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_ids_df1 = df1.User[df1.Rating.isna()].values\n",
    "print(movies_ids_df1)\n",
    "print(len(movies_ids_df1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿En qué formato está? Si queremos usarlo para pasar de identificador al nombre, debemos llevarlo a enteros. Asumimos que no hay ningun repetido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_ids_df1 = np.arange(1,len(movies_ids_df1) + 1)\n",
    "print(movies_ids_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Id\n",
    "\n",
    "Intentaremos agregar una columna al Dataframe con el Id de la película a la que corresponde la calificación. Para ello, vamos a necesitar saber dónde están ubicados los identificadores.\n",
    "\n",
    "Primero, seleccionamos los índices donde aparecen los movies_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_nan = pd.DataFrame(pd.isnull(df1.Rating))\n",
    "df1_nan = df1_nan[df1_nan['Rating'] == True]\n",
    "idx_movies_ids = df1_nan.index.values\n",
    "print(idx_movies_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos crear un vector de tantas instancias como df1, donde en cada lugar esté movie_id a cual corresponde la calificación. Como tenemos los índices donde está cada movie_id, podemos obtener cuántas calificaciones hay de cada película."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos el indice de la ultima instancia del dataframe\n",
    "idx_movies_ids = np.append(idx_movies_ids,df1.shape[0])\n",
    "cantidad_criticas = np.diff(idx_movies_ids)\n",
    "cantidad_criticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columna_movie_id = np.array([])\n",
    "for i in range(cantidad_criticas.size):\n",
    "    aux = np.full(cantidad_criticas[i], movies_ids_df1[i])\n",
    "    columna_movie_id = np.concatenate((columna_movie_id, aux))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregamos esa columna al dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['movie_id'] = columna_movie_id\n",
    "del columna_movie_id\n",
    "\n",
    "df1.dropna(inplace = True)\n",
    "df1['User'] = df1['User'].astype(int)\n",
    "df1['movie_id'] = df1['movie_id'].astype(np.int16)\n",
    "df1['Rating'] = df1['Rating'].astype(np.int8)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya contamos con un dataframe con calificaciones de usuarios a películas.\n",
    "\n",
    "Una opción es guardar el dataset modificado en nuevo archivo y, a partir de ahora, trabajar con esa versión. Esto hará que no tengamos que hacer el preprocesamiento cada vez que empecemos a trabajar y, además, ahorrarnos toda la \"basura\" que Python pueda ir dejando en la RAM.\n",
    "\n",
    "**Ejercicio**: guardar el dataset modificado en un nuevo archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    pass\n",
    "    #completar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploración del Dataset\n",
    "\n",
    "Responder las siguientes preguntas, siempre que se pueda con un lindo gráfico (¡pensar bien cómo!):\n",
    "\n",
    "1. ¿Cuántos usuarios únicos hay?\n",
    "2. ¿Cuántas películas calificó cada usuario?\n",
    "3. ¿Cómo es la distribución de las calificaciones?¿Pueden concluir algo de ese gráfico?\n",
    "4. ¿Cuál es la película con más calificaciones?¿Cuántas tiene?¿Y la que menos calificaciones tiene?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opcional: filtrar películas con pocos ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento\n",
    "\n",
    "Para entrenar el sistema de recomendación vamos a usar la biblioteca Surprise. Recomendamos tener abierta la [documentación](https://surprise.readthedocs.io/en/stable/getting_started.html) a medida que van a través de este notebook.\n",
    "\n",
    "### 3.1 Dataset y Train/test split\n",
    "\n",
    "Primero, llevamos el dataset al formato que le gusta a la biblioteca. ¿En qué orden tienen que estar los atributos?. Investigar qué hace la clase `Reader` y cuáles son sus parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, creamos el `Dataset` de Surprise usando `Dataset.load_from_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_filas = 1000000 # Limitamos el dataset a N_filas\n",
    "\n",
    "data = Dataset.load_from_df(df1[[COMPLETAR][:COMPLETAR], COMPLETAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo les parece que es mejor hacer el split?¿Dejando películas en test, usuarios o combinaciones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = COMPLETAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Entrenamiento\n",
    "\n",
    "Vamos a entrenar un algoritmo SVD. Explorar sus parámetros y su funcionamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "algo = SVD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos sobre el `trainset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.COMPLETAR(COMPLETAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y predecimos sobre el `testset`. Notar que para predecir sobre un conjunto de test se usa la función `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = algo.COMPLETAR(COMPLETAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explorar las característica de `predictions` y alguno de sus elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cambio, si queremos predecir para un usuario y una película en particular, usamos la función `predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.predict(237993,175)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploremos un usuario, veamos cuáles películas le gustaron y cuáles les recomienda el sistema.\n",
    "\n",
    "Priomero, buscamos qué películas le gustaron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usuario = 237993\n",
    "rating = 5   # le pedimos peliculas a las que haya puesto 4 o 5 estrellas\n",
    "df_user = df1[(df1['User'] == COMPLETAR) & (df1['Rating'] >= COMPLETAR)]\n",
    "df_user = df_user.reset_index(drop=True)\n",
    "df_user['Name'] = df_title['Name'].loc[df_user.movie_id].values\n",
    "df_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos donde vamos a guardar las recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomendaciones_usuario = df_title.iloc[:4499].copy()\n",
    "print(recomendaciones_usuario.shape)\n",
    "recomendaciones_usuario.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sacamos del dataframe todas las películas que ya sabemos que vio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usuario_vistas = df1[COMPLETAR]\n",
    "print(usuario_vistas.shape)\n",
    "\n",
    "recomendaciones_usuario.drop(COMPLETAR, inplace = True)\n",
    "recomendaciones_usuario = recomendaciones_usuario.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y hacemos las recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomendaciones_usuario['Estimate_Score'] = recomendaciones_usuario['Movie_Id'].apply(lambda x: algo.predict(usuario, x).est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomendaciones_usuario = recomendaciones_usuario.sort_values('Estimate_Score', ascending=False)\n",
    "print(recomendaciones_usuario.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evaluación\n",
    "\n",
    "Para el conjunto de `testset`, evaluamos el error RMSE entre las predicciones y las verdaderas calificaciones que le habían dado a las películas. Para eso, buscar en la documentación cómo se hace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import COMPLETAR\n",
    "\n",
    "COMPLETAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La próxima clase continuaremos con Optimización de Parámetros y algunos comentarios más sobre cómo trabajar con datasets grandes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
